{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vibration Fault Detection — ML Training\n",
    "\n",
    "Train tree-based classifiers on 35 firmware-matched features extracted from\n",
    "raw 3-axis acceleration segments.  Designed to run on **Google Colab** (no GPU needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cell 1: Setup ─────────────────────────────────────────────────────────\n# Install dependencies (uncomment on Colab)\n# !pip install -q numpy scipy pandas scikit-learn lightgbm xgboost matplotlib seaborn datasets torch pytorch-tabnet\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    roc_curve,\n    auc,\n)\nfrom sklearn.preprocessing import label_binarize, LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nimport torch\nimport torch.nn as nn\nimport pickle, os, warnings, copy\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Setup complete.  Device: {device}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cell 2: Configuration ─────────────────────────────────────────────────\n\n# ── Data source: choose ONE ──\n# Option A: HuggingFace dataset (for Colab)\nDATA_SOURCE = 'huggingface'\nHF_DATASET = 'adyady/bearing-fault-dataset'\nSIGNAL_FIELD = 'high_data'  # which array to use: 'high_data' or 'low_data'\n\n# Option B: Local JSON files\n# DATA_SOURCE = 'local'\n# DATA_DIR = 'data/raw'\n\n# Pre-computed feature CSV (skip extraction if it exists)\nFEATURE_CSV = 'features.csv'\n\n# Class grouping: map raw fault_category → training label\nCLASS_MAP = {\n    'healthy':         'Healthy',\n    'bearing_inner':   'Bearing Fault',\n    'bearing_outer':   'Bearing Fault',\n    'bearing_rolling': 'Bearing Fault',\n    'electrical':      'Electrical Fault',\n    'flow_cavitation': 'Flow/Cavitation',\n    'unbalance':       'Unbalance',\n    'misalignment':    'Misalignment',\n    'gear_fault':      'Gear Fault',\n}\n\n# 35 feature columns (canonical order from feature_extraction.py)\nFEATURE_COLS = [\n    'temp',\n    'xRMS', 'yRMS', 'zRMS',\n    'xVRMS', 'yVRMS', 'zVRMS',\n    'xEnvRMS', 'yEnvRMS', 'zEnvRMS',\n    'xKU', 'yKU', 'zKU',\n    'xP2P', 'yP2P', 'zP2P',\n    'maxCf',\n    'accLowPeakRatioX', 'accLowPeakRatioY', 'accLowPeakRatioZ',\n    'accMidPeakRatioX', 'accMidPeakRatioY', 'accMidPeakRatioZ',\n    'accHighPeakRatioX', 'accHighPeakRatioY', 'accHighPeakRatioZ',\n    'velLowPeakRatioX', 'velLowPeakRatioY', 'velLowPeakRatioZ',\n    'velMidPeakRatioX', 'velMidPeakRatioY', 'velMidPeakRatioZ',\n    'velHighPeakRatioX', 'velHighPeakRatioY', 'velHighPeakRatioZ',\n]\n\nRANDOM_STATE = 42\nprint(f'Config: {len(FEATURE_COLS)} features, source={DATA_SOURCE}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cell 3: Load Data ─────────────────────────────────────────────────────\n\nimport sys, json\n\n# --- Clone repo for feature_extraction.py if running on Colab ---\nif DATA_SOURCE == 'huggingface' and not os.path.exists('feature_extraction.py'):\n    print('feature_extraction.py not found locally.')\n    print('Paste it into Colab or upload it, then re-run this cell.')\n    print('(Or clone your repo: !git clone <your-repo-url>)')\n\nfrom feature_extraction import extract_features\n\nif os.path.exists(FEATURE_CSV):\n    print(f'Loading pre-computed features from {FEATURE_CSV}')\n    df = pd.read_csv(FEATURE_CSV)\n\nelif DATA_SOURCE == 'huggingface':\n    from datasets import load_dataset\n\n    print(f'Loading HuggingFace dataset: {HF_DATASET} ...')\n    ds = load_dataset(HF_DATASET, split='train')\n    hf_df = ds.to_pandas()\n    print(f'  Loaded {len(hf_df)} rows (each row = 1 axis of 1 segment)')\n    print(f'  Columns: {list(hf_df.columns)}')\n    print(f'  Axes: {hf_df[\"axis\"].value_counts().to_dict()}')\n    print(f'  Fault categories: {hf_df[\"fault_category\"].value_counts().to_dict()}')\n\n    # ── Group per-axis rows into segments ──\n    # Each unique file_name may have rows for axis x, y, z\n    grouped = hf_df.groupby('file_name')\n    print(f'\\n  Unique segments: {len(grouped)}')\n    print(f'  Extracting features (using \"{SIGNAL_FIELD}\" column) ...')\n\n    rows = []\n    for i, (seg_name, seg_rows) in enumerate(grouped):\n        try:\n            # Collect axes for this segment\n            axes_data = {}\n            for _, row in seg_rows.iterrows():\n                ax = row['axis']  # 'x', 'y', or 'z'\n                signal = np.asarray(row[SIGNAL_FIELD], dtype=np.float64)\n                axes_data[ax] = signal\n\n            x = axes_data.get('x')\n            y = axes_data.get('y')\n            z = axes_data.get('z')\n\n            if x is None:\n                continue\n\n            # Sample rate from the row\n            first_row = seg_rows.iloc[0]\n            fs = float(first_row.get('target_sample_rate') or first_row.get('original_sample_rate'))\n\n            # RPM from metadata_json if available\n            rpm = None\n            meta_str = first_row.get('metadata_json', '{}')\n            if isinstance(meta_str, str) and meta_str:\n                try:\n                    meta = json.loads(meta_str)\n                    rpm = meta.get('rpm') or meta.get('operating_conditions', {}).get('rpm')\n                except (json.JSONDecodeError, TypeError):\n                    pass\n            if rpm is not None:\n                rpm = float(rpm)\n\n            feats = extract_features(x, y, z, fs=fs, rpm=rpm)\n            feats['filename'] = seg_name\n            feats['fault_category'] = first_row.get('fault_category', '')\n            feats['fault_type'] = first_row.get('fault_type', '')\n            feats['dataset'] = first_row.get('source_dataset', '')\n            feats['sample_rate_hz'] = fs\n            rows.append(feats)\n\n        except Exception as e:\n            if i < 5:\n                print(f'    SKIP {seg_name}: {e}')\n\n        if (i + 1) % 2000 == 0:\n            print(f'    Processed {i + 1}/{len(grouped)} segments')\n\n    df = pd.DataFrame(rows)\n    df.to_csv(FEATURE_CSV, index=False)\n    print(f'\\n  Extracted {len(df)} segments → saved to {FEATURE_CSV}')\n\nelse:  # local JSON files\n    print(f'Running feature extraction on {DATA_DIR} ...')\n    from feature_extraction import extract_all\n    df = extract_all(DATA_DIR, output_csv=FEATURE_CSV)\n\n# ── Map fault_category → class label ──\ndf['label'] = df['fault_category'].map(CLASS_MAP)\nunknown = df['label'].isna().sum()\nif unknown > 0:\n    unmapped = df.loc[df['label'].isna(), 'fault_category'].unique()\n    print(f'WARNING: {unknown} rows have unmapped fault_category: {unmapped}')\n    print('         These rows will be dropped.  Update CLASS_MAP to include them.')\n    df = df.dropna(subset=['label'])\n\nprint(f'\\nDataset: {len(df)} segments, {df[\"label\"].nunique()} classes')\nprint(df['label'].value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: EDA ───────────────────────────────────────────────────────────\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# 4a. Class distribution\n",
    "order = df['label'].value_counts().index\n",
    "sns.countplot(data=df, y='label', order=order, ax=axes[0], hue='label', legend=False)\n",
    "axes[0].set_title('Class Distribution')\n",
    "axes[0].set_xlabel('Count')\n",
    "\n",
    "# 4b. Feature correlation heatmap (numeric features only, drop all-NaN cols)\n",
    "feat_present = [c for c in FEATURE_COLS if c in df.columns and df[c].notna().any()]\n",
    "corr = df[feat_present].corr()\n",
    "sns.heatmap(corr, ax=axes[1], cmap='coolwarm', center=0,\n",
    "            xticklabels=False, yticklabels=False, cbar_kws={'shrink': 0.6})\n",
    "axes[1].set_title('Feature Correlation')\n",
    "\n",
    "# 4c. Box plot of xRMS per class\n",
    "if 'xRMS' in df.columns:\n",
    "    sns.boxplot(data=df, y='label', x='xRMS', order=order, ax=axes[2], hue='label', legend=False)\n",
    "    axes[2].set_title('xRMS by Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4d. Additional box plots for key features\n",
    "key_features = ['xVRMS', 'xKU', 'xP2P', 'maxCf']\n",
    "key_features = [f for f in key_features if f in df.columns and df[f].notna().any()]\n",
    "if key_features:\n",
    "    fig, axes2 = plt.subplots(1, len(key_features), figsize=(5 * len(key_features), 4))\n",
    "    if len(key_features) == 1:\n",
    "        axes2 = [axes2]\n",
    "    for ax, feat in zip(axes2, key_features):\n",
    "        sns.boxplot(data=df, y='label', x=feat, order=order, ax=ax, hue='label', legend=False)\n",
    "        ax.set_title(feat)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cell 5: Preprocessing ─────────────────────────────────────────────────\n\n# Select feature columns that actually exist in the DataFrame\nuse_cols = [c for c in FEATURE_COLS if c in df.columns]\nX = df[use_cols].copy()\ny = df['label'].copy()\n\n# Train / test split (80/20, stratified)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n)\n\nprint(f'Features used: {len(use_cols)}')\nprint(f'Train: {len(X_train)}  |  Test: {len(X_test)}')\nprint(f'NaN fraction in train: {X_train.isna().mean().mean():.2%}')\nprint('No scaling applied for tree models.')\n\n# ── NN-specific preprocessing ──\n# Impute NaN → 0, then StandardScaler\nle = LabelEncoder()\nle.fit(y)\n\nscaler = StandardScaler()\nX_train_nn = pd.DataFrame(\n    scaler.fit_transform(X_train.fillna(0)),\n    columns=use_cols, index=X_train.index\n)\nX_test_nn = pd.DataFrame(\n    scaler.transform(X_test.fillna(0)),\n    columns=use_cols, index=X_test.index\n)\ny_train_enc = le.transform(y_train)\ny_test_enc = le.transform(y_test)\nnum_classes = len(le.classes_)\n\nprint(f'NN preprocessing: NaN→0, StandardScaler, {num_classes} classes encoded')\nprint(f'Classes: {list(le.classes_)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 6: Model Training ────────────────────────────────────────────────\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "class_labels = sorted(y.unique())\n",
    "\n",
    "# ── 6a. LightGBM (primary) ──\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=63,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=-1,\n",
    ")\n",
    "lgb_cv = cross_val_score(lgb_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "lgb_model.fit(X_train, y_train)\n",
    "print(f'LightGBM  5-fold CV accuracy: {lgb_cv.mean():.4f} ± {lgb_cv.std():.4f}')\n",
    "\n",
    "# ── 6b. XGBoost (comparison) ──\n",
    "# Compute sample weights for class imbalance\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "sw_train = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='mlogloss',\n",
    "    verbosity=0,\n",
    ")\n",
    "xgb_cv = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='accuracy',\n",
    "                          fit_params={'sample_weight': sw_train})\n",
    "xgb_model.fit(X_train, y_train, sample_weight=sw_train)\n",
    "print(f'XGBoost   5-fold CV accuracy: {xgb_cv.mean():.4f} ± {xgb_cv.std():.4f}')\n",
    "\n",
    "# ── 6c. Random Forest (baseline) ──\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=20,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_cv = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(f'RandomForest 5-fold CV accuracy: {rf_cv.mean():.4f} ± {rf_cv.std():.4f}')\n",
    "\n",
    "# Pick best model\n",
    "results = {\n",
    "    'LightGBM': (lgb_model, lgb_cv.mean()),\n",
    "    'XGBoost': (xgb_model, xgb_cv.mean()),\n",
    "    'RandomForest': (rf_model, rf_cv.mean()),\n",
    "}\n",
    "best_name = max(results, key=lambda k: results[k][1])\n",
    "best_model = results[best_name][0]\n",
    "print(f'\\nBest model: {best_name} ({results[best_name][1]:.4f} CV accuracy)')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ── Cell 6b: Residual MLP ─────────────────────────────────────────────────\n\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# ─────────────────────────────────────────────────────────────────────────\n# Architecture\n# ─────────────────────────────────────────────────────────────────────────\n\nclass ResidualBlock(nn.Module):\n    \"\"\"BN → GELU → Linear → BN → GELU → Dropout → Linear + skip.\"\"\"\n    def __init__(self, dim, dropout=0.3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.BatchNorm1d(dim),\n            nn.GELU(),\n            nn.Linear(dim, dim),\n            nn.BatchNorm1d(dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim, dim),\n        )\n    def forward(self, x):\n        return x + self.net(x)\n\n\nclass ResMLP(nn.Module):\n    \"\"\"Deep Residual MLP for tabular classification.\n\n    Input(n_features) → Linear(256) → BN → GELU → Dropout\n    → ResidualBlock × 4\n    → Linear(128) → BN → GELU → Dropout\n    → Linear(64)  → BN → GELU → Dropout\n    → Linear(n_classes)\n    \"\"\"\n    def __init__(self, n_features, n_classes, hidden=256, n_blocks=4, dropout=0.3):\n        super().__init__()\n        self.input_layer = nn.Sequential(\n            nn.Linear(n_features, hidden),\n            nn.BatchNorm1d(hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(hidden, dropout) for _ in range(n_blocks)]\n        )\n        self.head = nn.Sequential(\n            nn.Linear(hidden, hidden // 2),\n            nn.BatchNorm1d(hidden // 2),\n            nn.GELU(),\n            nn.Dropout(dropout * 0.7),\n            nn.Linear(hidden // 2, hidden // 4),\n            nn.BatchNorm1d(hidden // 4),\n            nn.GELU(),\n            nn.Dropout(dropout * 0.5),\n            nn.Linear(hidden // 4, n_classes),\n        )\n    def forward(self, x):\n        return self.head(self.res_blocks(self.input_layer(x)))\n\n\n# ─────────────────────────────────────────────────────────────────────────\n# Sklearn-compatible wrapper\n# ─────────────────────────────────────────────────────────────────────────\n\nclass TorchWrapper:\n    def __init__(self, model, label_encoder, device_):\n        self.model = model\n        self.le = label_encoder\n        self.device = device_\n\n    def predict_proba(self, X):\n        if isinstance(X, pd.DataFrame):\n            X = X.values\n        self.model.eval()\n        with torch.no_grad():\n            t = torch.tensor(X, dtype=torch.float32).to(self.device)\n            probs = torch.softmax(self.model(t), dim=1).cpu().numpy()\n        return probs\n\n    def predict(self, X):\n        probs = self.predict_proba(X)\n        return self.le.inverse_transform(np.argmax(probs, axis=1))\n\n\n# ─────────────────────────────────────────────────────────────────────────\n# Training function\n# ─────────────────────────────────────────────────────────────────────────\n\ndef train_resmlp(X_tr, y_tr, X_val, y_val, n_features, n_classes,\n                 class_weights, epochs=200, lr=1e-3, batch_size=256, patience=20):\n    model = ResMLP(n_features, n_classes, hidden=256, n_blocks=4, dropout=0.3).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2)\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n\n    train_ds = TensorDataset(\n        torch.tensor(X_tr, dtype=torch.float32),\n        torch.tensor(y_tr, dtype=torch.long),\n    )\n    loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_X = torch.tensor(X_val, dtype=torch.float32).to(device)\n    val_y = torch.tensor(y_val, dtype=torch.long).to(device)\n\n    best_loss, best_state, wait = float('inf'), None, 0\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            criterion(model(xb), yb).backward()\n            optimizer.step()\n        scheduler.step()\n\n        model.eval()\n        with torch.no_grad():\n            val_loss = criterion(model(val_X), val_y).item()\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_state = copy.deepcopy(model.state_dict())\n            wait = 0\n        else:\n            wait += 1\n            if wait >= patience:\n                break\n\n    model.load_state_dict(best_state)\n    # Final val accuracy\n    model.eval()\n    with torch.no_grad():\n        val_acc = (model(val_X).argmax(1) == val_y).float().mean().item()\n    print(f'    epoch {epoch+1}, val_loss={best_loss:.4f}, val_acc={val_acc:.4f}')\n    return model\n\n\n# ─────────────────────────────────────────────────────────────────────────\n# Train ResMLP\n# ─────────────────────────────────────────────────────────────────────────\n\ncw = compute_class_weight('balanced', classes=np.arange(num_classes), y=y_train_enc)\nclass_weights = torch.tensor(cw, dtype=torch.float32)\nprint(f'Class weights: {dict(zip(le.classes_, cw.round(2)))}')\n\nX_nn_np = X_train_nn.values\ny_nn_np = y_train_enc\n\n# 5-fold CV\nprint('\\n── Residual MLP (5-fold CV) ──')\ncv_accs = []\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X_nn_np, y_nn_np)):\n    print(f'  Fold {fold+1}/5:', end=' ')\n    m = train_resmlp(\n        X_nn_np[tr_idx], y_nn_np[tr_idx],\n        X_nn_np[val_idx], y_nn_np[val_idx],\n        n_features=len(use_cols), n_classes=num_classes,\n        class_weights=class_weights,\n    )\n    m.eval()\n    with torch.no_grad():\n        preds = m(torch.tensor(X_nn_np[val_idx], dtype=torch.float32).to(device)).argmax(1).cpu().numpy()\n    cv_accs.append((preds == y_nn_np[val_idx]).mean())\n\nresmlp_cv = np.mean(cv_accs)\nprint(f'ResMLP 5-fold CV accuracy: {resmlp_cv:.4f} ± {np.std(cv_accs):.4f}')\n\n# Final model on full training set\nprint('\\nTraining final ResMLP on full train set...')\nX_tr_f, X_val_f, y_tr_f, y_val_f = train_test_split(\n    X_nn_np, y_nn_np, test_size=0.1, random_state=RANDOM_STATE, stratify=y_nn_np\n)\nresmlp_final = train_resmlp(\n    X_tr_f, y_tr_f, X_val_f, y_val_f,\n    n_features=len(use_cols), n_classes=num_classes,\n    class_weights=class_weights, epochs=300, patience=30,\n)\nresmlp_wrapped = TorchWrapper(resmlp_final, le, device)\nresults['ResMLP'] = (resmlp_wrapped, resmlp_cv)\n\n# Update best model\nbest_name = max(results, key=lambda k: results[k][1])\nbest_model = results[best_name][0]\nprint(f'\\nBest overall: {best_name} ({results[best_name][1]:.4f})')\nfor name, (_, acc) in sorted(results.items(), key=lambda x: -x[1][1]):\n    print(f'  {name:<15s} {acc:.4f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ── Cell 7: Evaluation ────────────────────────────────────────────────────\n\nfor name, (model, _) in results.items():\n    # NN models need scaled input; tree models use raw\n    if name in ('ResMLP', 'TabNet'):\n        y_pred = model.predict(X_test_nn)\n    else:\n        y_pred = model.predict(X_test)\n    acc = (y_pred == y_test).mean()\n    print(f'\\n{\"=\" * 60}')\n    print(f'{name}  —  Test Accuracy: {acc:.4f}')\n    print(f'{\"=\" * 60}')\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n# Confusion matrix for best model\nif best_name in ('ResMLP', 'TabNet'):\n    y_pred_best = best_model.predict(X_test_nn)\nelse:\n    y_pred_best = best_model.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred_best, labels=class_labels)\nfig, ax = plt.subplots(figsize=(8, 6))\nConfusionMatrixDisplay(cm, display_labels=class_labels).plot(ax=ax, cmap='Blues', colorbar=False)\nax.set_title(f'Confusion Matrix — {best_name}')\nplt.xticks(rotation=30, ha='right')\nplt.tight_layout()\nplt.show()\n\n# ROC curves (one-vs-rest)\nif hasattr(best_model, 'predict_proba'):\n    y_test_bin = label_binarize(y_test, classes=class_labels)\n    if best_name in ('ResMLP', 'TabNet'):\n        y_score = best_model.predict_proba(X_test_nn)\n    else:\n        y_score = best_model.predict_proba(X_test)\n    fig, ax = plt.subplots(figsize=(8, 6))\n    for i, cls in enumerate(class_labels):\n        if y_test_bin.shape[1] > i:\n            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n            ax.plot(fpr, tpr, label=f'{cls} (AUC={auc(fpr, tpr):.3f})')\n    ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n    ax.set_xlabel('FPR')\n    ax.set_ylabel('TPR')\n    ax.set_title(f'ROC Curves — {best_name}')\n    ax.legend(loc='lower right', fontsize=8)\n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 8: Feature Importance ────────────────────────────────────────────\n",
    "\n",
    "importance = lgb_model.feature_importances_\n",
    "feat_imp = pd.Series(importance, index=use_cols).sort_values(ascending=False)\n",
    "\n",
    "top_n = min(20, len(feat_imp))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "feat_imp.head(top_n).plot.barh(ax=ax)\n",
    "ax.invert_yaxis()\n",
    "ax.set_title(f'Top-{top_n} Feature Importance (LightGBM)')\n",
    "ax.set_xlabel('Importance (split count)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nTop-20 features:')\n",
    "print(feat_imp.head(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 9: Save Model ────────────────────────────────────────────────────\n",
    "\n",
    "artifact = {\n",
    "    'model': best_model,\n",
    "    'model_name': best_name,\n",
    "    'feature_names': use_cols,\n",
    "    'class_labels': class_labels,\n",
    "    'class_map': CLASS_MAP,\n",
    "    'cv_accuracy': results[best_name][1],\n",
    "}\n",
    "\n",
    "model_path = 'best_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(artifact, f)\n",
    "\n",
    "print(f'Saved {best_name} to {model_path}')\n",
    "print(f'  Features : {len(use_cols)}')\n",
    "print(f'  Classes  : {class_labels}')\n",
    "print(f'  CV acc   : {results[best_name][1]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}