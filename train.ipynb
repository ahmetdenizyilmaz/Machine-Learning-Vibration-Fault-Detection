{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vibration Fault Detection — ML Training\n",
    "\n",
    "Train tree-based classifiers on 35 firmware-matched features extracted from\n",
    "raw 3-axis acceleration segments.  Designed to run on **Google Colab** (no GPU needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cell 1: Setup ─────────────────────────────────────────────────────────\n# Install dependencies (uncomment on Colab)\n# !pip install -q numpy scipy pandas scikit-learn lightgbm xgboost matplotlib seaborn datasets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    roc_curve,\n    auc,\n)\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nimport pickle, os, warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nprint('Setup complete.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cell 2: Configuration ─────────────────────────────────────────────────\n\n# ── Data source: choose ONE ──\n# Option A: HuggingFace dataset (for Colab)\nDATA_SOURCE = 'huggingface'\nHF_DATASET = 'adyady/bearing-fault-dataset'\nSIGNAL_FIELD = 'high_data'  # which array to use: 'high_data' or 'low_data'\n\n# Option B: Local JSON files\n# DATA_SOURCE = 'local'\n# DATA_DIR = 'data/raw'\n\n# Pre-computed feature CSV (skip extraction if it exists)\nFEATURE_CSV = 'features.csv'\n\n# Class grouping: map raw fault_category → training label\nCLASS_MAP = {\n    'healthy':         'Healthy',\n    'bearing_inner':   'Bearing Fault',\n    'bearing_outer':   'Bearing Fault',\n    'bearing_rolling': 'Bearing Fault',\n    'electrical':      'Electrical Fault',\n    'flow_cavitation': 'Flow/Cavitation',\n    'unbalance':       'Unbalance',\n    'misalignment':    'Misalignment',\n    'gear_fault':      'Gear Fault',\n}\n\n# 35 feature columns (canonical order from feature_extraction.py)\nFEATURE_COLS = [\n    'temp',\n    'xRMS', 'yRMS', 'zRMS',\n    'xVRMS', 'yVRMS', 'zVRMS',\n    'xEnvRMS', 'yEnvRMS', 'zEnvRMS',\n    'xKU', 'yKU', 'zKU',\n    'xP2P', 'yP2P', 'zP2P',\n    'maxCf',\n    'accLowPeakRatioX', 'accLowPeakRatioY', 'accLowPeakRatioZ',\n    'accMidPeakRatioX', 'accMidPeakRatioY', 'accMidPeakRatioZ',\n    'accHighPeakRatioX', 'accHighPeakRatioY', 'accHighPeakRatioZ',\n    'velLowPeakRatioX', 'velLowPeakRatioY', 'velLowPeakRatioZ',\n    'velMidPeakRatioX', 'velMidPeakRatioY', 'velMidPeakRatioZ',\n    'velHighPeakRatioX', 'velHighPeakRatioY', 'velHighPeakRatioZ',\n]\n\nRANDOM_STATE = 42\nprint(f'Config: {len(FEATURE_COLS)} features, source={DATA_SOURCE}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Cell 3: Load Data ─────────────────────────────────────────────────────\n\nimport sys, json\n\n# --- Clone repo for feature_extraction.py if running on Colab ---\nif DATA_SOURCE == 'huggingface' and not os.path.exists('feature_extraction.py'):\n    print('feature_extraction.py not found locally.')\n    print('Paste it into Colab or upload it, then re-run this cell.')\n    print('(Or clone your repo: !git clone <your-repo-url>)')\n\nfrom feature_extraction import extract_features\n\nif os.path.exists(FEATURE_CSV):\n    print(f'Loading pre-computed features from {FEATURE_CSV}')\n    df = pd.read_csv(FEATURE_CSV)\n\nelif DATA_SOURCE == 'huggingface':\n    from datasets import load_dataset\n\n    print(f'Loading HuggingFace dataset: {HF_DATASET} ...')\n    ds = load_dataset(HF_DATASET, split='train')\n    hf_df = ds.to_pandas()\n    print(f'  Loaded {len(hf_df)} rows (each row = 1 axis of 1 segment)')\n    print(f'  Columns: {list(hf_df.columns)}')\n    print(f'  Axes: {hf_df[\"axis\"].value_counts().to_dict()}')\n    print(f'  Fault categories: {hf_df[\"fault_category\"].value_counts().to_dict()}')\n\n    # ── Group per-axis rows into segments ──\n    # Each unique file_name may have rows for axis x, y, z\n    grouped = hf_df.groupby('file_name')\n    print(f'\\n  Unique segments: {len(grouped)}')\n    print(f'  Extracting features (using \"{SIGNAL_FIELD}\" column) ...')\n\n    rows = []\n    for i, (seg_name, seg_rows) in enumerate(grouped):\n        try:\n            # Collect axes for this segment\n            axes_data = {}\n            for _, row in seg_rows.iterrows():\n                ax = row['axis']  # 'x', 'y', or 'z'\n                signal = np.asarray(row[SIGNAL_FIELD], dtype=np.float64)\n                axes_data[ax] = signal\n\n            x = axes_data.get('x')\n            y = axes_data.get('y')\n            z = axes_data.get('z')\n\n            if x is None:\n                continue\n\n            # Sample rate from the row\n            first_row = seg_rows.iloc[0]\n            fs = float(first_row.get('target_sample_rate') or first_row.get('original_sample_rate'))\n\n            # RPM from metadata_json if available\n            rpm = None\n            meta_str = first_row.get('metadata_json', '{}')\n            if isinstance(meta_str, str) and meta_str:\n                try:\n                    meta = json.loads(meta_str)\n                    rpm = meta.get('rpm') or meta.get('operating_conditions', {}).get('rpm')\n                except (json.JSONDecodeError, TypeError):\n                    pass\n            if rpm is not None:\n                rpm = float(rpm)\n\n            feats = extract_features(x, y, z, fs=fs, rpm=rpm)\n            feats['filename'] = seg_name\n            feats['fault_category'] = first_row.get('fault_category', '')\n            feats['fault_type'] = first_row.get('fault_type', '')\n            feats['dataset'] = first_row.get('source_dataset', '')\n            feats['sample_rate_hz'] = fs\n            rows.append(feats)\n\n        except Exception as e:\n            if i < 5:\n                print(f'    SKIP {seg_name}: {e}')\n\n        if (i + 1) % 2000 == 0:\n            print(f'    Processed {i + 1}/{len(grouped)} segments')\n\n    df = pd.DataFrame(rows)\n    df.to_csv(FEATURE_CSV, index=False)\n    print(f'\\n  Extracted {len(df)} segments → saved to {FEATURE_CSV}')\n\nelse:  # local JSON files\n    print(f'Running feature extraction on {DATA_DIR} ...')\n    from feature_extraction import extract_all\n    df = extract_all(DATA_DIR, output_csv=FEATURE_CSV)\n\n# ── Map fault_category → class label ──\ndf['label'] = df['fault_category'].map(CLASS_MAP)\nunknown = df['label'].isna().sum()\nif unknown > 0:\n    unmapped = df.loc[df['label'].isna(), 'fault_category'].unique()\n    print(f'WARNING: {unknown} rows have unmapped fault_category: {unmapped}')\n    print('         These rows will be dropped.  Update CLASS_MAP to include them.')\n    df = df.dropna(subset=['label'])\n\nprint(f'\\nDataset: {len(df)} segments, {df[\"label\"].nunique()} classes')\nprint(df['label'].value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: EDA ───────────────────────────────────────────────────────────\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# 4a. Class distribution\n",
    "order = df['label'].value_counts().index\n",
    "sns.countplot(data=df, y='label', order=order, ax=axes[0], hue='label', legend=False)\n",
    "axes[0].set_title('Class Distribution')\n",
    "axes[0].set_xlabel('Count')\n",
    "\n",
    "# 4b. Feature correlation heatmap (numeric features only, drop all-NaN cols)\n",
    "feat_present = [c for c in FEATURE_COLS if c in df.columns and df[c].notna().any()]\n",
    "corr = df[feat_present].corr()\n",
    "sns.heatmap(corr, ax=axes[1], cmap='coolwarm', center=0,\n",
    "            xticklabels=False, yticklabels=False, cbar_kws={'shrink': 0.6})\n",
    "axes[1].set_title('Feature Correlation')\n",
    "\n",
    "# 4c. Box plot of xRMS per class\n",
    "if 'xRMS' in df.columns:\n",
    "    sns.boxplot(data=df, y='label', x='xRMS', order=order, ax=axes[2], hue='label', legend=False)\n",
    "    axes[2].set_title('xRMS by Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4d. Additional box plots for key features\n",
    "key_features = ['xVRMS', 'xKU', 'xP2P', 'maxCf']\n",
    "key_features = [f for f in key_features if f in df.columns and df[f].notna().any()]\n",
    "if key_features:\n",
    "    fig, axes2 = plt.subplots(1, len(key_features), figsize=(5 * len(key_features), 4))\n",
    "    if len(key_features) == 1:\n",
    "        axes2 = [axes2]\n",
    "    for ax, feat in zip(axes2, key_features):\n",
    "        sns.boxplot(data=df, y='label', x=feat, order=order, ax=ax, hue='label', legend=False)\n",
    "        ax.set_title(feat)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 5: Preprocessing ─────────────────────────────────────────────────\n",
    "\n",
    "# Select feature columns that actually exist in the DataFrame\n",
    "use_cols = [c for c in FEATURE_COLS if c in df.columns]\n",
    "X = df[use_cols].copy()\n",
    "y = df['label'].copy()\n",
    "\n",
    "# Train / test split (80/20, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Features used: {len(use_cols)}')\n",
    "print(f'Train: {len(X_train)}  |  Test: {len(X_test)}')\n",
    "print(f'NaN fraction in train: {X_train.isna().mean().mean():.2%}')\n",
    "print('\\nNo scaling applied — tree models handle raw values and NaN natively.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 6: Model Training ────────────────────────────────────────────────\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "class_labels = sorted(y.unique())\n",
    "\n",
    "# ── 6a. LightGBM (primary) ──\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=63,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=-1,\n",
    ")\n",
    "lgb_cv = cross_val_score(lgb_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "lgb_model.fit(X_train, y_train)\n",
    "print(f'LightGBM  5-fold CV accuracy: {lgb_cv.mean():.4f} ± {lgb_cv.std():.4f}')\n",
    "\n",
    "# ── 6b. XGBoost (comparison) ──\n",
    "# Compute sample weights for class imbalance\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "sw_train = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='mlogloss',\n",
    "    verbosity=0,\n",
    ")\n",
    "xgb_cv = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='accuracy',\n",
    "                          fit_params={'sample_weight': sw_train})\n",
    "xgb_model.fit(X_train, y_train, sample_weight=sw_train)\n",
    "print(f'XGBoost   5-fold CV accuracy: {xgb_cv.mean():.4f} ± {xgb_cv.std():.4f}')\n",
    "\n",
    "# ── 6c. Random Forest (baseline) ──\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=20,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_cv = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(f'RandomForest 5-fold CV accuracy: {rf_cv.mean():.4f} ± {rf_cv.std():.4f}')\n",
    "\n",
    "# Pick best model\n",
    "results = {\n",
    "    'LightGBM': (lgb_model, lgb_cv.mean()),\n",
    "    'XGBoost': (xgb_model, xgb_cv.mean()),\n",
    "    'RandomForest': (rf_model, rf_cv.mean()),\n",
    "}\n",
    "best_name = max(results, key=lambda k: results[k][1])\n",
    "best_model = results[best_name][0]\n",
    "print(f'\\nBest model: {best_name} ({results[best_name][1]:.4f} CV accuracy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 7: Evaluation ────────────────────────────────────────────────────\n",
    "\n",
    "for name, (model, _) in results.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = (y_pred == y_test).mean()\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'{name}  —  Test Accuracy: {acc:.4f}')\n",
    "    print(f'{\"=\" * 60}')\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Confusion matrix for best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=class_labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay(cm, display_labels=class_labels).plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "ax.set_title(f'Confusion Matrix — {best_name}')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curves (one-vs-rest)\n",
    "if hasattr(best_model, 'predict_proba'):\n",
    "    y_test_bin = label_binarize(y_test, classes=class_labels)\n",
    "    y_score = best_model.predict_proba(X_test)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    for i, cls in enumerate(class_labels):\n",
    "        if y_test_bin.shape[1] > i:\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "            ax.plot(fpr, tpr, label=f'{cls} (AUC={auc(fpr, tpr):.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_ylabel('TPR')\n",
    "    ax.set_title(f'ROC Curves — {best_name}')\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 8: Feature Importance ────────────────────────────────────────────\n",
    "\n",
    "importance = lgb_model.feature_importances_\n",
    "feat_imp = pd.Series(importance, index=use_cols).sort_values(ascending=False)\n",
    "\n",
    "top_n = min(20, len(feat_imp))\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "feat_imp.head(top_n).plot.barh(ax=ax)\n",
    "ax.invert_yaxis()\n",
    "ax.set_title(f'Top-{top_n} Feature Importance (LightGBM)')\n",
    "ax.set_xlabel('Importance (split count)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nTop-20 features:')\n",
    "print(feat_imp.head(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 9: Save Model ────────────────────────────────────────────────────\n",
    "\n",
    "artifact = {\n",
    "    'model': best_model,\n",
    "    'model_name': best_name,\n",
    "    'feature_names': use_cols,\n",
    "    'class_labels': class_labels,\n",
    "    'class_map': CLASS_MAP,\n",
    "    'cv_accuracy': results[best_name][1],\n",
    "}\n",
    "\n",
    "model_path = 'best_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(artifact, f)\n",
    "\n",
    "print(f'Saved {best_name} to {model_path}')\n",
    "print(f'  Features : {len(use_cols)}')\n",
    "print(f'  Classes  : {class_labels}')\n",
    "print(f'  CV acc   : {results[best_name][1]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}