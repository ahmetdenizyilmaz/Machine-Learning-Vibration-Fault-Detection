{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmetdenizyilmaz/Machine-Learning-Vibration-Fault-Detection/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JskNtyt8Ryr3"
      },
      "source": [
        "# Vibration Fault Detection — ResMLP Training\n",
        "\n",
        "Train a **Residual MLP** classifier on 35 firmware-matched features extracted from\n",
        "raw 3-axis acceleration segments.  Designed to run on **Google Colab** (GPU recommended)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AeuNG1C-Ryr4",
        "outputId": "9d7ac447-1403-4ed0-e2c4-6c09b324fd97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete.  Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ── Cell 1: Setup ─────────────────────────────────────────────────────────\n",
        "# Install dependencies (uncomment on Colab)\n",
        "# !pip install -q numpy scipy pandas scikit-learn matplotlib seaborn datasets torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import copy\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    roc_curve,\n",
        "    auc,\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize, LabelEncoder, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "import pickle, os, json, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Setup complete.  Device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iFs1S8mIR7un",
        "outputId": "7302ccce-d35d-4703-c98b-26d2310cb549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U2adpvhRyr5"
      },
      "outputs": [],
      "source": [
        "# ── Cell 2: Configuration ─────────────────────────────────────────────────\n",
        "\n",
        "# ── Data source: choose ONE ──\n",
        "# Option A: HuggingFace dataset (for Colab)\n",
        "DATA_SOURCE = 'huggingface'\n",
        "HF_DATASET = 'adyady/bearing-fault-dataset'\n",
        "SIGNAL_FIELD = 'low_data'  # which array to use: 'high_data' or 'low_data'\n",
        "\n",
        "# Option B: Local JSON files\n",
        "# DATA_SOURCE = 'local'\n",
        "# DATA_DIR = 'data/raw'\n",
        "\n",
        "# Pre-computed feature CSV (skip extraction if it exists)\n",
        "FEATURE_CSV = 'features.csv'\n",
        "\n",
        "# Class grouping: map raw fault_category → training label\n",
        "CLASS_MAP = {\n",
        "    'healthy':          'Healthy',\n",
        "    'bearing_inner':    'Bearing Fault',\n",
        "    'bearing_outer':    'Bearing Fault',\n",
        "    'bearing_rolling':  'Bearing Fault',\n",
        "    'bearing_fault':    'Bearing Fault',\n",
        "    'bearing_combined': 'Bearing Fault',\n",
        "    'electrical':       'Electrical Fault',\n",
        "    'flow_cavitation':  'Flow/Cavitation',\n",
        "    'unbalance':        'Unbalance',\n",
        "    'misalignment':     'Misalignment',\n",
        "    'eccentricity':     'Misalignment',\n",
        "    'gear_fault':       'Gear Fault',\n",
        "    'rotor_fault':      'Rotor Fault',\n",
        "    'unknown':          'Unknown',\n",
        "}\n",
        "\n",
        "# 35 feature columns (canonical order from feature_extraction.py)\n",
        "FEATURE_COLS = [\n",
        "    'temp',\n",
        "    'xRMS', 'yRMS', 'zRMS',\n",
        "    'xVRMS', 'yVRMS', 'zVRMS',\n",
        "    'xEnvRMS', 'yEnvRMS', 'zEnvRMS',\n",
        "    'xKU', 'yKU', 'zKU',\n",
        "    'xP2P', 'yP2P', 'zP2P',\n",
        "    'maxCf',\n",
        "    'accLowPeakRatioX', 'accLowPeakRatioY', 'accLowPeakRatioZ',\n",
        "    'accMidPeakRatioX', 'accMidPeakRatioY', 'accMidPeakRatioZ',\n",
        "    'accHighPeakRatioX', 'accHighPeakRatioY', 'accHighPeakRatioZ',\n",
        "    'velLowPeakRatioX', 'velLowPeakRatioY', 'velLowPeakRatioZ',\n",
        "    'velMidPeakRatioX', 'velMidPeakRatioY', 'velMidPeakRatioZ',\n",
        "    'velHighPeakRatioX', 'velHighPeakRatioY', 'velHighPeakRatioZ',\n",
        "]\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "print(f'Config: {len(FEATURE_COLS)} features, source={DATA_SOURCE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4ZiTR_8Ryr5"
      },
      "outputs": [],
      "source": [
        "# ── Cell 3: Load Data ─────────────────────────────────────────────────────\n",
        "\n",
        "import sys, json\n",
        "\n",
        "# --- Clone repo for feature_extraction.py if running on Colab ---\n",
        "if DATA_SOURCE == 'huggingface' and not os.path.exists('feature_extraction.py'):\n",
        "    print('feature_extraction.py not found locally.')\n",
        "    print('Paste it into Colab or upload it, then re-run this cell.')\n",
        "    print('(Or clone your repo: !git clone <your-repo-url>)')\n",
        "\n",
        "from feature_extraction import extract_features\n",
        "\n",
        "if os.path.exists(FEATURE_CSV):\n",
        "    print(f'Loading pre-computed features from {FEATURE_CSV}')\n",
        "    df = pd.read_csv(FEATURE_CSV)\n",
        "\n",
        "elif DATA_SOURCE == 'huggingface':\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    print(f'Loading HuggingFace dataset: {HF_DATASET} ...')\n",
        "    ds = load_dataset(HF_DATASET, split='train')\n",
        "    hf_df = ds.to_pandas()\n",
        "    print(f'  Loaded {len(hf_df)} rows (each row = 1 axis of 1 segment)')\n",
        "    print(f'  Columns: {list(hf_df.columns)}')\n",
        "    print(f'  Axes: {hf_df[\"axis\"].value_counts().to_dict()}')\n",
        "    print(f'  Fault categories: {hf_df[\"fault_category\"].value_counts().to_dict()}')\n",
        "\n",
        "    # ── Group per-axis rows into segments ──\n",
        "    # Each unique file_name may have rows for axis x, y, z\n",
        "    grouped = hf_df.groupby('file_name')\n",
        "    print(f'\\n  Unique segments: {len(grouped)}')\n",
        "    print(f'  Extracting features (using \"{SIGNAL_FIELD}\" column) ...')\n",
        "\n",
        "    rows = []\n",
        "    for i, (seg_name, seg_rows) in enumerate(grouped):\n",
        "        try:\n",
        "            # Collect axes for this segment\n",
        "            axes_data = {}\n",
        "            for _, row in seg_rows.iterrows():\n",
        "                ax = row['axis']  # 'x', 'y', or 'z'\n",
        "                signal = np.asarray(row[SIGNAL_FIELD], dtype=np.float64)\n",
        "                axes_data[ax] = signal\n",
        "\n",
        "            x = axes_data.get('x')\n",
        "            y = axes_data.get('y')\n",
        "            z = axes_data.get('z')\n",
        "\n",
        "            if x is None:\n",
        "                continue\n",
        "\n",
        "            # Sample rate from the row\n",
        "            first_row = seg_rows.iloc[0]\n",
        "            fs = float(first_row.get('target_sample_rate') or first_row.get('original_sample_rate'))\n",
        "\n",
        "            # RPM from metadata_json if available\n",
        "            rpm = None\n",
        "            meta_str = first_row.get('metadata_json', '{}')\n",
        "            if isinstance(meta_str, str) and meta_str:\n",
        "                try:\n",
        "                    meta = json.loads(meta_str)\n",
        "                    rpm = meta.get('rpm') or meta.get('operating_conditions', {}).get('rpm')\n",
        "                except (json.JSONDecodeError, TypeError):\n",
        "                    pass\n",
        "            if rpm is not None:\n",
        "                rpm = float(rpm)\n",
        "\n",
        "            feats = extract_features(x, y, z, fs=fs, rpm=rpm)\n",
        "            feats['filename'] = seg_name\n",
        "            feats['fault_category'] = first_row.get('fault_category', '')\n",
        "            feats['fault_type'] = first_row.get('fault_type', '')\n",
        "            feats['dataset'] = first_row.get('source_dataset', '')\n",
        "            feats['sample_rate_hz'] = fs\n",
        "            rows.append(feats)\n",
        "\n",
        "        except Exception as e:\n",
        "            if i < 5:\n",
        "                print(f'    SKIP {seg_name}: {e}')\n",
        "\n",
        "        if (i + 1) % 2000 == 0:\n",
        "            print(f'    Processed {i + 1}/{len(grouped)} segments')\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(FEATURE_CSV, index=False)\n",
        "    print(f'\\n  Extracted {len(df)} segments → saved to {FEATURE_CSV}')\n",
        "\n",
        "else:  # local JSON files\n",
        "    print(f'Running feature extraction on {DATA_DIR} ...')\n",
        "    from feature_extraction import extract_all\n",
        "    df = extract_all(DATA_DIR, output_csv=FEATURE_CSV)\n",
        "\n",
        "# ── Map fault_category → class label ──\n",
        "df['label'] = df['fault_category'].map(CLASS_MAP)\n",
        "unknown = df['label'].isna().sum()\n",
        "if unknown > 0:\n",
        "    unmapped = df.loc[df['label'].isna(), 'fault_category'].unique()\n",
        "    print(f'WARNING: {unknown} rows have unmapped fault_category: {unmapped}')\n",
        "    print('         These rows will be dropped.  Update CLASS_MAP to include them.')\n",
        "    df = df.dropna(subset=['label'])\n",
        "\n",
        "print(f'\\nDataset: {len(df)} segments, {df[\"label\"].nunique()} classes')\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDwWSOiQRyr5"
      },
      "outputs": [],
      "source": [
        "# ── Cell 4: EDA ───────────────────────────────────────────────────────────\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "# 4a. Class distribution\n",
        "order = df['label'].value_counts().index\n",
        "sns.countplot(data=df, y='label', order=order, ax=axes[0], hue='label', legend=False)\n",
        "axes[0].set_title('Class Distribution')\n",
        "axes[0].set_xlabel('Count')\n",
        "\n",
        "# 4b. Feature correlation heatmap (numeric features only, drop all-NaN cols)\n",
        "feat_present = [c for c in FEATURE_COLS if c in df.columns and df[c].notna().any()]\n",
        "corr = df[feat_present].corr()\n",
        "sns.heatmap(corr, ax=axes[1], cmap='coolwarm', center=0,\n",
        "            xticklabels=False, yticklabels=False, cbar_kws={'shrink': 0.6})\n",
        "axes[1].set_title('Feature Correlation')\n",
        "\n",
        "# 4c. Box plot of xRMS per class\n",
        "if 'xRMS' in df.columns:\n",
        "    sns.boxplot(data=df, y='label', x='xRMS', order=order, ax=axes[2], hue='label', legend=False)\n",
        "    axes[2].set_title('xRMS by Class')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4d. Additional box plots for key features\n",
        "key_features = ['xVRMS', 'xKU', 'xP2P', 'maxCf']\n",
        "key_features = [f for f in key_features if f in df.columns and df[f].notna().any()]\n",
        "if key_features:\n",
        "    fig, axes2 = plt.subplots(1, len(key_features), figsize=(5 * len(key_features), 4))\n",
        "    if len(key_features) == 1:\n",
        "        axes2 = [axes2]\n",
        "    for ax, feat in zip(axes2, key_features):\n",
        "        sns.boxplot(data=df, y='label', x=feat, order=order, ax=ax, hue='label', legend=False)\n",
        "        ax.set_title(feat)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ1NjyacRyr5"
      },
      "outputs": [],
      "source": [
        "# ── Cell 5: Preprocessing ─────────────────────────────────────────────────\n",
        "\n",
        "# Select feature columns that actually exist in the DataFrame\n",
        "use_cols = [c for c in FEATURE_COLS if c in df.columns]\n",
        "X = df[use_cols].copy()\n",
        "y = df['label'].copy()\n",
        "\n",
        "# Train / test split (80/20, stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "le.fit(y)\n",
        "y_train_enc = le.transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "num_classes = len(le.classes_)\n",
        "class_labels = list(le.classes_)\n",
        "\n",
        "# Impute NaN → 0, then StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_sc = scaler.fit_transform(X_train.fillna(0))\n",
        "X_test_sc = scaler.transform(X_test.fillna(0))\n",
        "\n",
        "print(f'Features used: {len(use_cols)}')\n",
        "print(f'Train: {len(X_train)}  |  Test: {len(X_test)}')\n",
        "print(f'NaN fraction in train: {X_train.isna().mean().mean():.2%}')\n",
        "print(f'Classes ({num_classes}): {class_labels}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gabsF-SRyr6"
      },
      "outputs": [],
      "source": [
        "# ── Cell 6: ResMLP — Architecture & Training ─────────────────────────────\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "# Architecture\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"BN → GELU → Linear → BN → GELU → Dropout → Linear + skip.\"\"\"\n",
        "    def __init__(self, dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.BatchNorm1d(dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.BatchNorm1d(dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.net(x)\n",
        "\n",
        "\n",
        "class ResMLP(nn.Module):\n",
        "    \"\"\"Deep Residual MLP for tabular classification.\n",
        "\n",
        "    Input(n_features) → Linear(256) → BN → GELU → Dropout\n",
        "    → ResidualBlock × 4\n",
        "    → Linear(128) → BN → GELU → Dropout\n",
        "    → Linear(64)  → BN → GELU → Dropout\n",
        "    → Linear(n_classes)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_classes, hidden=256, n_blocks=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Linear(n_features, hidden),\n",
        "            nn.BatchNorm1d(hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            *[ResidualBlock(hidden, dropout) for _ in range(n_blocks)]\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden // 2),\n",
        "            nn.BatchNorm1d(hidden // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout * 0.7),\n",
        "            nn.Linear(hidden // 2, hidden // 4),\n",
        "            nn.BatchNorm1d(hidden // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout * 0.5),\n",
        "            nn.Linear(hidden // 4, n_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.head(self.res_blocks(self.input_layer(x)))\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "# Sklearn-compatible wrapper (for evaluation & saving)\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class TorchWrapper:\n",
        "    def __init__(self, model, scaler, label_encoder, feature_names, device_):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.le = label_encoder\n",
        "        self.feature_names = feature_names\n",
        "        self.device = device_\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            t = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "            probs = torch.softmax(self.model(t), dim=1).cpu().numpy()\n",
        "        return probs\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return self.le.inverse_transform(np.argmax(probs, axis=1))\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "# Training function\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def train_resmlp(X_tr, y_tr, X_val, y_val, n_features, n_classes,\n",
        "                 class_weights, epochs=200, lr=1e-3, batch_size=256, patience=20):\n",
        "    model = ResMLP(n_features, n_classes, hidden=256, n_blocks=4, dropout=0.3).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "    train_ds = TensorDataset(\n",
        "        torch.tensor(X_tr, dtype=torch.float32),\n",
        "        torch.tensor(y_tr, dtype=torch.long),\n",
        "    )\n",
        "    loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_X = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "    val_y = torch.tensor(y_val, dtype=torch.long).to(device)\n",
        "\n",
        "    best_loss, best_state, wait = float('inf'), None, 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            criterion(model(xb), yb).backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = criterion(model(val_X), val_y).item()\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_acc = (model(val_X).argmax(1) == val_y).float().mean().item()\n",
        "    return model, epoch + 1, best_loss, val_acc\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "# Class weights for imbalanced data\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "cw = compute_class_weight('balanced', classes=np.arange(num_classes), y=y_train_enc)\n",
        "class_weights = torch.tensor(cw, dtype=torch.float32)\n",
        "print(f'Class weights: {dict(zip(le.classes_, cw.round(2)))}')\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "# 5-fold Cross-Validation\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print('\\n── ResMLP 5-fold CV ──')\n",
        "cv_accs = []\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_sc, y_train_enc)):\n",
        "    model, ep, loss, acc = train_resmlp(\n",
        "        X_train_sc[tr_idx], y_train_enc[tr_idx],\n",
        "        X_train_sc[val_idx], y_train_enc[val_idx],\n",
        "        n_features=len(use_cols), n_classes=num_classes,\n",
        "        class_weights=class_weights,\n",
        "    )\n",
        "    cv_accs.append(acc)\n",
        "    print(f'  Fold {fold+1}/5: epoch {ep}, val_loss={loss:.4f}, val_acc={acc:.4f}')\n",
        "\n",
        "print(f'\\nResMLP 5-fold CV accuracy: {np.mean(cv_accs):.4f} ± {np.std(cv_accs):.4f}')\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "# Train final model on full training set\n",
        "# ─────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "print('\\nTraining final ResMLP on full train set...')\n",
        "X_tr_f, X_val_f, y_tr_f, y_val_f = train_test_split(\n",
        "    X_train_sc, y_train_enc, test_size=0.1, random_state=RANDOM_STATE, stratify=y_train_enc\n",
        ")\n",
        "final_model, ep, loss, acc = train_resmlp(\n",
        "    X_tr_f, y_tr_f, X_val_f, y_val_f,\n",
        "    n_features=len(use_cols), n_classes=num_classes,\n",
        "    class_weights=class_weights, epochs=300, patience=30,\n",
        ")\n",
        "print(f'Final model: epoch {ep}, val_loss={loss:.4f}, val_acc={acc:.4f}')\n",
        "\n",
        "best_model = TorchWrapper(final_model, scaler, le, use_cols, device)\n",
        "best_cv_acc = np.mean(cv_accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKOfjFUARyr6"
      },
      "outputs": [],
      "source": [
        "# ── Cell 7: Evaluation ────────────────────────────────────────────────────\n",
        "\n",
        "y_pred = best_model.predict(X_test_sc)\n",
        "y_proba = best_model.predict_proba(X_test_sc)\n",
        "test_acc = (y_pred == y_test.values).mean()\n",
        "\n",
        "print(f'ResMLP Test Accuracy: {test_acc:.4f}')\n",
        "print(f'CV Accuracy:          {best_cv_acc:.4f} ± {np.std(cv_accs):.4f}')\n",
        "print()\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# ── Confusion Matrix ──\n",
        "cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ConfusionMatrixDisplay(cm, display_labels=class_labels).plot(ax=ax, cmap='Blues', colorbar=False)\n",
        "ax.set_title(f'Confusion Matrix — ResMLP (acc={test_acc:.3f})')\n",
        "plt.xticks(rotation=30, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ── ROC Curves (one-vs-rest) ──\n",
        "y_test_bin = label_binarize(y_test, classes=class_labels)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "for i, cls in enumerate(class_labels):\n",
        "    if y_test_bin.shape[1] > i:\n",
        "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
        "        ax.plot(fpr, tpr, label=f'{cls} (AUC={auc(fpr, tpr):.3f})')\n",
        "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "ax.set_xlabel('FPR')\n",
        "ax.set_ylabel('TPR')\n",
        "ax.set_title('ROC Curves — ResMLP')\n",
        "ax.legend(loc='lower right', fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruHx-XuuRyr6"
      },
      "outputs": [],
      "source": [
        "# ── Cell 8: Feature Importance (Permutation) ─────────────────────────────\n",
        "# Permutation importance: shuffle each feature and measure accuracy drop.\n",
        "# Works with any model (no LightGBM needed).\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class _SklearnBridge(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"Minimal bridge so sklearn's permutation_importance works with TorchWrapper.\"\"\"\n",
        "    def __init__(self, wrapper):\n",
        "        self.wrapper = wrapper\n",
        "        self.classes_ = wrapper.le.classes_\n",
        "    def fit(self, X, y): return self\n",
        "    def predict(self, X): return self.wrapper.predict(X)\n",
        "    def score(self, X, y): return (self.predict(X) == y).mean()\n",
        "\n",
        "bridge = _SklearnBridge(best_model)\n",
        "print('Computing permutation importance (this may take a minute)...')\n",
        "result = permutation_importance(\n",
        "    bridge, X_test_sc, y_test.values,\n",
        "    n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1,\n",
        ")\n",
        "\n",
        "feat_imp = pd.Series(result.importances_mean, index=use_cols).sort_values(ascending=False)\n",
        "\n",
        "top_n = min(20, len(feat_imp))\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "feat_imp.head(top_n).plot.barh(ax=ax)\n",
        "ax.invert_yaxis()\n",
        "ax.set_title(f'Top-{top_n} Feature Importance (Permutation)')\n",
        "ax.set_xlabel('Mean accuracy decrease')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nTop-20 features:')\n",
        "print(feat_imp.head(20).to_string())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Cell 9: Save Model ────────────────────────────────────────────────────\n",
        "\n",
        "# Save PyTorch state dict + metadata\n",
        "artifact = {\n",
        "    'model_state_dict': final_model.state_dict(),\n",
        "    'model_config': {\n",
        "        'n_features': len(use_cols),\n",
        "        'n_classes': num_classes,\n",
        "        'hidden': 256,\n",
        "        'n_blocks': 4,\n",
        "        'dropout': 0.3,\n",
        "    },\n",
        "    'scaler': scaler,\n",
        "    'label_encoder': le,\n",
        "    'feature_names': use_cols,\n",
        "    'class_labels': class_labels,\n",
        "    'class_map': CLASS_MAP,\n",
        "    'cv_accuracy': best_cv_acc,\n",
        "    'test_accuracy': test_acc,\n",
        "}\n",
        "\n",
        "model_path = 'resmlp_model.pkl'\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(artifact, f)\n",
        "\n",
        "print(f'Saved ResMLP to {model_path}')\n",
        "print(f'  Features : {len(use_cols)}')\n",
        "print(f'  Classes  : {class_labels}')\n",
        "print(f'  CV acc   : {best_cv_acc:.4f}')\n",
        "print(f'  Test acc : {test_acc:.4f}')\n",
        "\n",
        "# Also save as pure PyTorch checkpoint\n",
        "torch_path = 'resmlp_checkpoint.pt'\n",
        "torch.save({\n",
        "    'model_state_dict': final_model.state_dict(),\n",
        "    'model_config': artifact['model_config'],\n",
        "    'class_labels': class_labels,\n",
        "    'feature_names': use_cols,\n",
        "}, torch_path)\n",
        "print(f'  PyTorch checkpoint: {torch_path}')"
      ],
      "metadata": {
        "id": "5dUtVg9mRyr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}